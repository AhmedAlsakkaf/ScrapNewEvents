{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d97f9768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b875c5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_10times_events():\n",
    "    \"\"\"\n",
    "    Scrape medical-pharma events from 10times.com\n",
    "    Returns a list of dictionaries containing event data\n",
    "    \"\"\"\n",
    "    \n",
    "    url = \"https://10times.com/usa/medical-pharma\"\n",
    "    \n",
    "    # Headers to mimic a real browser\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "    }\n",
    "    \n",
    "    print(f\"Fetching data from: {url}\")\n",
    "    \n",
    "    try:\n",
    "        # Make the request\n",
    "        response = requests.get(url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        print(f\"Successfully fetched page (Status: {response.status_code})\")\n",
    "        \n",
    "        # Parse the HTML\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find all event cards based on the structure you provided\n",
    "        event_cards = soup.find_all('tr', class_=lambda x: x and 'event-card' in x)\n",
    "        print(f\"Found {len(event_cards)} event cards\")\n",
    "        \n",
    "        events_data = []\n",
    "        \n",
    "        for i, card in enumerate(event_cards, 1):\n",
    "            try:\n",
    "                event_data = extract_event_data(card)\n",
    "                if event_data:\n",
    "                    events_data.append(event_data)\n",
    "                    print(f\"Processed event {i}: {event_data.get('event_name', 'Unknown')[:50]}...\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing event {i}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return events_data\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching the page: {str(e)}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac99eeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_event_data(card):\n",
    "    \"\"\"\n",
    "    Extract event data from a single event card\n",
    "    \"\"\"\n",
    "    event_data = {}\n",
    "    \n",
    "    try:\n",
    "        # Extract event date from the first td\n",
    "        date_td = card.find('td', class_='text-dark')\n",
    "        if date_td:\n",
    "            event_data['event_date'] = date_td.get_text(strip=True)\n",
    "        else:\n",
    "            event_data['event_date'] = 'N/A'\n",
    "        \n",
    "        # Extract event name and link from the onclick attribute\n",
    "        clickable_td = card.find('td', {'onclick': True})\n",
    "        if clickable_td:\n",
    "            onclick_content = clickable_td.get('onclick', '')\n",
    "            # Extract URL from onclick=\"window.open('URL')\"\n",
    "            url_match = re.search(r\"window\\.open\\(['\\\"]([^'\\\"]+)['\\\"]\", onclick_content)\n",
    "            if url_match:\n",
    "                event_data['event_link'] = url_match.group(1)\n",
    "                # Extract event name from the URL (last part after the last slash)\n",
    "                event_name = event_data['event_link'].split('/')[-1].replace('-', ' ').title()\n",
    "                event_data['event_name'] = event_name\n",
    "            else:\n",
    "                event_data['event_link'] = 'N/A'\n",
    "                event_data['event_name'] = 'N/A'\n",
    "        else:\n",
    "            event_data['event_link'] = 'N/A'\n",
    "            event_data['event_name'] = 'N/A'\n",
    "        \n",
    "        # Extract venue/city from the venue div\n",
    "        venue_link = card.find('div', class_='venue')\n",
    "        if venue_link:\n",
    "            venue_a = venue_link.find('a')\n",
    "            if venue_a:\n",
    "                event_data['venue_city'] = venue_a.get_text(strip=True)\n",
    "            else:\n",
    "                event_data['venue_city'] = venue_link.get_text(strip=True)\n",
    "        else:\n",
    "            event_data['venue_city'] = 'N/A'\n",
    "        \n",
    "        # Extract description from the text-wrap div\n",
    "        description_div = card.find('div', class_='text-wrap text-break')\n",
    "        if description_div:\n",
    "            event_data['description'] = description_div.get_text(strip=True)\n",
    "        else:\n",
    "            event_data['description'] = 'N/A'\n",
    "        \n",
    "        # Extract categories/tags from the spans with bg-light class\n",
    "        categories = []\n",
    "        category_spans = card.find_all('span', class_='bg-light rounded')\n",
    "        for span in category_spans:\n",
    "            categories.append(span.get_text(strip=True))\n",
    "        \n",
    "        # Also look for links in the same td for additional categories\n",
    "        category_links = card.find_all('a', {'rel': 'nofollow'})\n",
    "        for link in category_links:\n",
    "            categories.append(link.get_text(strip=True))\n",
    "        \n",
    "        event_data['categories_tags'] = ', '.join(categories) if categories else 'N/A'\n",
    "        \n",
    "        # Extract interested count from the last td\n",
    "        footer_td = card.find('td', class_='tb-foot')\n",
    "        if footer_td:\n",
    "            interested_link = footer_td.find('a', class_='xn')\n",
    "            if interested_link:\n",
    "                event_data['interested_count'] = interested_link.get_text(strip=True)\n",
    "            else:\n",
    "                event_data['interested_count'] = '0'\n",
    "        else:\n",
    "            event_data['interested_count'] = '0'\n",
    "        \n",
    "        return event_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting event data: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12a44826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(events_data, filename='10times_medical_pharma_events.csv'):\n",
    "    \"\"\"\n",
    "    Save the scraped events data to a CSV file\n",
    "    \"\"\"\n",
    "    if not events_data:\n",
    "        print(\"No data to save!\")\n",
    "        return\n",
    "    \n",
    "    # Define the CSV headers based on the fields we're extracting\n",
    "    headers = [\n",
    "        'event_date',\n",
    "        'event_name', \n",
    "        'event_link',\n",
    "        'venue_city',\n",
    "        'description',\n",
    "        'categories_tags',\n",
    "        'interested_count'\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(events_data)\n",
    "        \n",
    "        print(f\"Successfully saved {len(events_data)} events to {filename}\")\n",
    "        \n",
    "        # Also display as pandas DataFrame for quick preview\n",
    "        df = pd.DataFrame(events_data)\n",
    "        print(f\"\\nPreview of the data:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to CSV: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2af2f0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 10times.com Medical & Pharma Events Scraper\n",
      "==================================================\n",
      "Fetching data from: https://10times.com/usa/medical-pharma\n",
      "Error fetching the page: 403 Client Error: Forbidden for url: https://10times.com/usa/medical-pharma\n",
      "No events were scraped. Please check the website structure or your internet connection.\n",
      "\n",
      "==================================================\n",
      "Scraping process completed!\n",
      "Error fetching the page: 403 Client Error: Forbidden for url: https://10times.com/usa/medical-pharma\n",
      "No events were scraped. Please check the website structure or your internet connection.\n",
      "\n",
      "==================================================\n",
      "Scraping process completed!\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting 10times.com Medical & Pharma Events Scraper\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Scrape the events\n",
    "    events = scrape_10times_events()\n",
    "    \n",
    "    if events:\n",
    "        print(f\"\\nSuccessfully scraped {len(events)} events!\")\n",
    "        \n",
    "        # Save to CSV\n",
    "        df = save_to_csv(events)\n",
    "        \n",
    "        if df is not None:\n",
    "            print(\"\\nScraping completed successfully!\")\n",
    "            print(f\"Data saved to: 10times_medical_pharma_events.csv\")\n",
    "            print(f\"Total events scraped: {len(events)}\")\n",
    "            \n",
    "            # Display summary statistics\n",
    "            print(f\"\\nSummary:\")\n",
    "            print(f\"- Events with venue information: {df['venue_city'].notna().sum()}\")\n",
    "            print(f\"- Events with descriptions: {df['description'].notna().sum()}\")\n",
    "            print(f\"- Events with categories: {df['categories_tags'].notna().sum()}\")\n",
    "            print(f\"- Total interested count: {df['interested_count'].astype(str).str.extract(r'(\\d+)', expand=False).fillna('0').astype(int).sum()}\")\n",
    "        else:\n",
    "            print(\"Failed to save data to CSV!\")\n",
    "    else:\n",
    "        print(\"No events were scraped. Please check the website structure or your internet connection.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Scraping process completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "786a9a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Test with a small sample first (uncomment to use)\n",
    "# def test_scraper():\n",
    "#     \"\"\"\n",
    "#     Test function to debug the scraper with sample HTML\n",
    "#     \"\"\"\n",
    "#     sample_html = '''\n",
    "#     <tr class=\"row py-2 mb-3 bg-white deep-shadow event-card event_189360\">\n",
    "#       <td class=\"col-12 text-dark\" data-localizers=\"ignore\" data-start-date=\"2025/10/16\"\n",
    "#           data-status=\"active\" data-date-format=\"default\" style=\"line-height: 1.2;\"\n",
    "#           data-time-diff=\"-313\">\n",
    "#         Sun, 12 â€“ Thu, 16 Oct 2025\n",
    "#       </td>\n",
    "#       <td class=\"col-12 c-ga cursor-pointer text-break show-related\" data-id=\"189360\"\n",
    "#           onclick=\"window.open('https://10times.com/ecs-meetings-chicago')\">\n",
    "#         <div class=\"col-12 mb-2\">\n",
    "#           <div class=\"small fw-500 venue\">\n",
    "#             <a class=\"text-dark text-decoration-none\" href=\"https://10times.com/chicago-us/medical-pharma\">\n",
    "#               Chicago\n",
    "#             </a>\n",
    "#           </div>\n",
    "#         </div>\n",
    "#         <div class=\"col-12 mt-3\">\n",
    "#           <div class=\"small text-wrap text-break\" style=\"color:#5e5e5e; line-height:1.2;\">\n",
    "#             ECS Meetings bring together global scientists, engineers, and industry leaders...\n",
    "#           </div>\n",
    "#         </div>\n",
    "#       </td>\n",
    "#       <td class=\"col-12 small text-muted-new mb-2\" style=\"line-height:1.2;\">\n",
    "#         <span class=\"d-inline-block small me-2 p-1 lh-1 bg-light rounded\">Conference</span>\n",
    "#         <span class=\"d-inline-block small me-2 p-1 lh-1 bg-light rounded\">Medical & Pharma</span>\n",
    "#       </td>\n",
    "#       <td class=\"col-12 mt-3 mb-1 tb-foot\">\n",
    "#         <div class=\"d-flex justify-content-between align-items-center\">\n",
    "#           <a class=\"fw-500 text-decoration-none mx-2 xn\" \n",
    "#              href=\"https://10times.com/ecs-meetings-chicago/visitors\"\n",
    "#              target=\"_blank\" rel=\"noreferrer\">\n",
    "#             6\n",
    "#           </a>\n",
    "#         </div>\n",
    "#       </td>\n",
    "#     </tr>\n",
    "#     '''\n",
    "#     \n",
    "#     soup = BeautifulSoup(sample_html, 'html.parser')\n",
    "#     card = soup.find('tr', class_=lambda x: x and 'event-card' in x)\n",
    "#     \n",
    "#     if card:\n",
    "#         result = extract_event_data(card)\n",
    "#         print(\"Test result:\", result)\n",
    "#         return result\n",
    "#     else:\n",
    "#         print(\"No card found in test HTML\")\n",
    "#         return None\n",
    "\n",
    "# Uncomment the line below to run the test\n",
    "# test_scraper()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
